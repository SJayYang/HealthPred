{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FP9g3sXH9klX"
      },
      "outputs": [],
      "source": [
        "import os "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir(\"drive/MyDrive/bmi/\")"
      ],
      "metadata": {
        "id": "8QdeLrbq9pGF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_sn6kGAR9-9R",
        "outputId": "9fe1ad1e-723f-445d-a7ac-f3eed92d4b96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.25.1-py3-none-any.whl (5.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.8 MB 15.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 55.7 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.10.0\n",
            "  Downloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n",
            "\u001b[K     |████████████████████████████████| 182 kB 73.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.9.24)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.11.1 tokenizers-0.13.2 transformers-4.25.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os \n",
        "import argparse \n",
        "import random \n",
        "import pandas as pd\n",
        "import numpy as np \n",
        "import collections \n",
        "from tqdm import tqdm \n",
        "from datetime import datetime\n",
        "\n",
        "# PyTorch libraries \n",
        "import torch \n",
        "import torch.nn as nn \n",
        "from torch.utils.data import DataLoader \n",
        "from torchvision.datasets import DatasetFolder\n",
        "from torchvision import models, transforms \n",
        "\n",
        "# Hugging Face datasets \n",
        "#import datasets \n",
        "\n",
        "# Transformers libraries \n",
        "from transformers import TrainingArguments, Trainer\n",
        "from transformers import ViTFeatureExtractor, ViTForImageClassification\n",
        "from transformers import get_linear_schedule_with_warmup \n",
        "\n",
        "# import sklearn models \n",
        "# from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "# simple models\n",
        "from models import LogisticRegression, BasicCNNModel, DenseCNNModel, BasicCNNCountryModel, ViTCountryModel, ViTMosaiksModel\n",
        "from SatelliteImageDataset import SatelliteImageDataset, SatelliteImageMetadataDataset, SatelliteImageMosaiksDataset\n",
        "\n",
        "from sklearn.metrics import confusion_matrix"
      ],
      "metadata": {
        "id": "-E5-YORX-JJ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "RANDOM_SEED = 231 \n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "np.random.seed(RANDOM_SEED)\n",
        "random.seed(RANDOM_SEED)\n",
        "\n",
        "class ImageClassificationCollator:\n",
        "    def __init__(self, feature_extractor, transforms = False, metadata = False, mosaiks = False): \n",
        "        self.feature_extractor = feature_extractor\n",
        "        self.transforms = transforms \n",
        "        self.metadata = metadata\n",
        "        self.mosaiks = mosaiks\n",
        "\n",
        "    def __call__(self, batch):\n",
        "        if self.transforms: \n",
        "            transformed = [self.feature_extractor(x[0].cpu().detach().numpy()) for x in batch]\n",
        "            encodings = {\"pixel_values\":torch.stack(transformed)}\n",
        "        else: \n",
        "            encodings = self.feature_extractor([x[0] for x in batch], return_tensors='pt')   \n",
        "        encodings['labels'] = torch.tensor([x[1] for x in batch],  dtype=torch.long)\n",
        "        \n",
        "        if self.metadata: \n",
        "            if \"country\" in self.metadata:\n",
        "                encodings['country'] = torch.tensor([x[2] for x in batch])\n",
        "        elif self.mosaiks: \n",
        "            encodings['mosaiks_features'] = torch.tensor(np.array([x[2] for x in batch]), dtype = torch.float32)\n",
        "\n",
        "        return encodings\n",
        "\n",
        "# create model and collator\n",
        "def create_model_and_collator(args, model_name, metadata = None, cnt_id_map = None):\n",
        "\n",
        "    if metadata:\n",
        "        feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224-in21k')\n",
        "        collator = ImageClassificationCollator(feature_extractor, metadata=metadata)\n",
        "        collators = (collator, collator)\n",
        "        if model_name in ['basic_cnn']:\n",
        "            model = BasicCNNCountryModel(n_classes=CLASSES, cnt_id_map = cnt_id_map, num_country_embeddings=len(cnt_id_map))\n",
        "        elif model_name == \"ViT\":\n",
        "            model = ViTCountryModel(n_classes=CLASSES, cnt_id_map = cnt_id_map, num_country_embeddings=len(cnt_id_map))\n",
        "    elif args['mosaiks']: \n",
        "        feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224-in21k')\n",
        "        collator = ImageClassificationCollator(feature_extractor, mosaiks=args['mosaiks'])\n",
        "        collators = (collator, collator)\n",
        "        model = ViTMosaiksModel(n_classes=CLASSES, mosaiks_dim = 3999)\n",
        "    elif model_name == \"ViT\":\n",
        "        feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224-in21k')\n",
        "        collator = ImageClassificationCollator(feature_extractor)\n",
        "        collators = (collator, collator)\n",
        "        model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224-in21k', num_labels=CLASSES)\n",
        "\n",
        "    elif model_name in ['resnet', 'alexnet', 'vgg', 'squeezenet', 'densenet']:\n",
        "        # note all models expect image of (3, 224, 224)\n",
        "\n",
        "        train_data_transforms = transforms.Compose([\n",
        "            transforms.ToPILImage(),\n",
        "            transforms.RandomResizedCrop(224), # i.e. want 224 by 224 \n",
        "            transforms.RandomHorizontalFlip(),  \n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        ])\n",
        "        val_data_transforms = transforms.Compose([\n",
        "            transforms.ToPILImage(),\n",
        "            transforms.Resize(224), # i.e. want 224 by 224 \n",
        "            transforms.CenterCrop(224), \n",
        "            transforms.ToTensor(), \n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        ])\n",
        "        train_collator = ImageClassificationCollator(train_data_transforms, transforms=True)\n",
        "        val_collator = ImageClassificationCollator(val_data_transforms, transforms=True)\n",
        "\n",
        "        collators = (train_collator, val_collator)\n",
        "\n",
        "        if model_name == 'resnet':\n",
        "            model = models.resnet18(pretrained=True)\n",
        "            model.fc = nn.Linear(model.fc.in_features, CLASSES)\n",
        "\n",
        "        elif model_name == 'alexnet':\n",
        "            model = models.alexnet(pretrained=True)\n",
        "            model.classifier[6] = nn.Linear(model.classifier[6].in_features, CLASSES)\n",
        "\n",
        "        elif model_name == 'vgg':\n",
        "            model = models.vgg11_bn(pretrained=True)\n",
        "            model.classifier[6] = nn.Linear(model.classifier[6].in_features, CLASSES)\n",
        "\n",
        "        elif model_name == 'squeezenet': \n",
        "            model = models.squeezenet1_0(pretrained=True)\n",
        "            model.classifier[1] = nn.Conv2d(512, CLASSES, kernel_size=1, stride=1)\n",
        "            model.num_classes = CLASSES\n",
        "\n",
        "        else: \n",
        "            # dense net \n",
        "            model = models.densenet121(pretrained=True)\n",
        "            model.classifier = nn.Linear(model.classifier.in_features, CLASSES) \n",
        "\n",
        "    elif model_name in ['basic_cnn', 'basic_cnn_novit', 'dense_cnn', 'logistic_regression']:\n",
        "        # ADD IN transforms though feature extractor might be easier \n",
        "        if \"novit\" in model_name:\n",
        "            train_data_transforms = transforms.Compose([\n",
        "                transforms.ToPILImage(),\n",
        "                transforms.RandomResizedCrop(224), # i.e. want 224 by 224 \n",
        "                transforms.RandomHorizontalFlip(),  \n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "            ])\n",
        "            val_data_transforms = transforms.Compose([\n",
        "                transforms.ToPILImage(),\n",
        "                transforms.Resize(224), # i.e. want 224 by 224 \n",
        "                transforms.CenterCrop(224),\n",
        "                transforms.ToTensor(), \n",
        "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "            ])\n",
        "            train_collator = ImageClassificationCollator(train_data_transforms, transforms=True)\n",
        "            val_collator = ImageClassificationCollator(val_data_transforms, transforms=True)\n",
        "\n",
        "            collators = (train_collator, val_collator)\n",
        "        else: \n",
        "            feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224-in21k')\n",
        "            collator = ImageClassificationCollator(feature_extractor)\n",
        "            collators = (collator, collator)\n",
        "        # TODO: add support for model!\n",
        "        if model_name == \"logistic_regression\":\n",
        "            model = LogisticRegression(n_classes=CLASSES)\n",
        "        elif \"basic_cnn\" in model_name:\n",
        "            model = BasicCNNModel(n_classes=CLASSES)\n",
        "        elif model_name == \"dense_cnn\":\n",
        "            model = DenseCNNModel(n_classes=CLASSES)\n",
        "\n",
        "    else: \n",
        "        raise NotImplementedError\n",
        "\n",
        "    print(f'Model name: {model_name}')\n",
        "\n",
        "    return collators, model \n",
        "\n",
        "\n",
        "def create_dataset(args, collator_fns, metadata = None, cnt_id_map = None, val_split = 0.15):\n",
        "\n",
        "    def npy_loader(path):\n",
        "        sample = torch.from_numpy(np.load(path))\n",
        "        return sample \n",
        "    \n",
        "    # load in dataset frmom directory \n",
        "    if metadata:\n",
        "        dataset = SatelliteImageMetadataDataset(\n",
        "            root = args['data_dir'], \n",
        "            csv_path = args['csv_file'], \n",
        "            outcome = args['outcome'], \n",
        "            loader = npy_loader\n",
        "        )\n",
        "    elif args['mosaiks']: \n",
        "        dataset = SatelliteImageMosaiksDataset(\n",
        "            root = args['data_dir'], \n",
        "            csv_path = args['csv_file'], \n",
        "            outcome = args['outcome'], \n",
        "            mosaiks_csv_path = args['mosaiks_csv_file'], \n",
        "            loader = npy_loader\n",
        "        )\n",
        "    else:\n",
        "        dataset = SatelliteImageDataset(\n",
        "            root = args['data_dir'], \n",
        "            csv_path = args['csv_file'],\n",
        "            outcome = args['outcome'], \n",
        "            loader = npy_loader\n",
        "        )\n",
        "\n",
        "    # IDEALLY we would like same sampling...\n",
        "\n",
        "    # split up into train val data \n",
        "    if os.path.isfile(\"indices_perm2.npy\"):\n",
        "        indices = np.load(\"indices_perm2.npy\") \n",
        "    else:\n",
        "        indices = torch.randperm(len(dataset)).tolist()\n",
        "        np.save(\"indices_perm2.npy\", indices)\n",
        "    \n",
        "    n_val = int(np.floor(len(indices) * val_split))\n",
        "    train_ds = torch.utils.data.Subset(dataset, indices[:-n_val])\n",
        "    val_ds = torch.utils.data.Subset(dataset, indices[-n_val:])\n",
        "\n",
        "    train_dl = DataLoader(train_ds, batch_size=args['batch_size'], collate_fn=collator_fns[0], shuffle = 1)\n",
        "    val_dl = DataLoader(val_ds, batch_size=args['batch_size'], collate_fn=collator_fns[1], shuffle=0)\n",
        "\n",
        "    return [train_dl, val_dl]\n",
        "\n",
        "def dataset_statistics(args, dataset_loader):\n",
        "    label_stats = collections.Counter()\n",
        "    for i, batch in enumerate(dataset_loader):\n",
        "        inputs, labels = batch['pixel_values'], batch['labels']\n",
        "        labels = labels.cpu().numpy().flatten()\n",
        "        label_stats += collections.Counter(labels)\n",
        "    return label_stats\n",
        "\n",
        "\n",
        "def measure_accuracy(outputs, labels):\n",
        "    preds = np.argmax(outputs, axis = 1).flatten()\n",
        "    labels = labels.flatten()\n",
        "    correct = np.sum(preds == labels)\n",
        "    c_matrix = confusion_matrix(labels, preds, labels=CLASS_NAMES)\n",
        "    return correct, len(labels), c_matrix \n",
        "\n",
        "def validation(args, val_loader, model, criterion, metadata, device, name = 'Validation', write_file=None):\n",
        "\n",
        "    model.eval()\n",
        "    total_loss = 0. \n",
        "    total_correct = 0 \n",
        "    total_sample = 0 \n",
        "    total_confusion = np.zeros((CLASSES, CLASSES))\n",
        "\n",
        "    for i, batch in enumerate(tqdm(val_loader)):\n",
        "        inputs, labels = batch['pixel_values'], batch['labels'] \n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            if metadata: \n",
        "                country = batch['country'].to(device)\n",
        "                outputs = model(inputs, country)\n",
        "            elif args['mosaiks']: \n",
        "                mosaiks_features = batch['mosaiks_features'].to(device)\n",
        "                outputs = model(inputs, mosaiks_features)\n",
        "            elif args['model_name'] in [\n",
        "            'basic_cnn', 'basic_cnn_novit', 'dense_cnn', 'logistic_regression',\n",
        "            'resnet', 'alexnet', 'vgg', 'squeezenet', 'densenet'\n",
        "            ]:\n",
        "                outputs = model(inputs)\n",
        "            else: \n",
        "                outputs = model(inputs)['logits'] \n",
        "\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        logits = outputs \n",
        "        total_loss += loss.cpu().item()\n",
        "\n",
        "        correct_n, sample_n, c_matrix = measure_accuracy(logits.cpu().numpy(), labels.cpu().numpy())\n",
        "        total_correct += correct_n \n",
        "        total_sample += sample_n \n",
        "        total_confusion += c_matrix \n",
        "\n",
        "    print(f'*** Accuracy on the {name} set: {total_correct/total_sample}')\n",
        "    print(f'*** Weighted accuracy on the {name} set: {np.mean( np.diag(total_confusion) / np.sum(total_confusion, 1) )}')\n",
        "    print(f'*** Confusion matrix:\\n{total_confusion}')\n",
        "    if write_file:\n",
        "        write_file.write(f'*** Accuracy on the {name} set: {total_correct/total_sample}\\n')\n",
        "        write_file.write(f'*** Weighted accuracy on the {name} set: {np.mean( np.diag(total_confusion) / np.sum(total_confusion, 1) )}')\n",
        "        write_file.write(f'*** Confusion matrix:\\n{total_confusion}\\n')\n",
        "\n",
        "    return total_loss, float(total_correct / total_sample) * 100\n",
        "\n",
        "\n",
        "\n",
        "def train(args, data_loaders, epoch_n, model, optim, scheduler, criterion, metadata, device, write_file=None):\n",
        "    print(\"\\n>>> Training starts...\")\n",
        "\n",
        "    if write_file: \n",
        "        write_file.write(\"\\n>>> Training starts...\")\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    best_val_acc = 0\n",
        "    for epoch in range(epoch_n):\n",
        "        print(\"*** Epoch:\", epoch)\n",
        "        total_train_loss = 0. \n",
        "        total_correct = 0\n",
        "        total_sample = 0\n",
        "\n",
        "        for i, batch in enumerate(tqdm(data_loaders[0])): \n",
        "            optim.zero_grad()\n",
        "            inputs, labels = batch['pixel_values'], batch['labels'] \n",
        "            inputs = inputs.to(device, non_blocking=True)\n",
        "            labels = labels.to(device, non_blocking=True)\n",
        "\n",
        "            # forward pass \n",
        "            if metadata: \n",
        "                country = batch['country'].to(device)\n",
        "                outputs = model(inputs, country)\n",
        "            elif args['mosaiks']: \n",
        "                mosaiks_features = batch['mosaiks_features'].to(device)\n",
        "                outputs = model(inputs, mosaiks_features)\n",
        "            elif args['model_name'] in [\n",
        "                'basic_cnn', 'basic_cnn_novit', 'dense_cnn', 'logistic_regression', \n",
        "                'resnet', 'alexnet', 'vgg', 'squeezenet', 'densenet'\n",
        "            ]:\n",
        "                outputs = model(inputs)\n",
        "            else: \n",
        "                outputs = model(inputs)['logits']\n",
        "\n",
        "            loss = criterion(outputs, labels)\n",
        "            logits = outputs\n",
        "            correct_n, sample_n, c_matrix = measure_accuracy(logits.cpu().detach().numpy(), labels.cpu().detach().numpy())\n",
        "            total_correct += correct_n \n",
        "            total_sample += sample_n \n",
        "\n",
        "            total_train_loss += loss.cpu().item()\n",
        "\n",
        "            # backward pass \n",
        "            loss.backward()\n",
        "            optim.step()\n",
        "\n",
        "            if scheduler: scheduler.step()\n",
        "\n",
        "            if i % args['val_every'] == 0: \n",
        "                print(f'*** Average Loss: {total_train_loss / (i+1)}')\n",
        "                print(f'*** Running accuracy on the train set: {total_correct/total_sample}')\n",
        "                if write_file:\n",
        "                    write_file.write(f'\\nEpoch: {epoch}, Step: {i}\\n')\n",
        "                    write_file.write(f'*** Loss: {loss}\\n')\n",
        "                    write_file.write(f'*** Running accuracy on the train set: {total_correct/total_sample}\\n')\n",
        "\n",
        "                _, val_acc = validation(args, data_loaders[1], model, criterion, metadata, device, write_file=write_file)\n",
        "\n",
        "                model.train()\n",
        "\n",
        "                if best_val_acc < val_acc: \n",
        "                    best_val_acc = val_acc \n",
        "\n",
        "                    if args['save_path']:\n",
        "                        if args['mosaiks']:\n",
        "                          with open(args['save_path'] + \".pkl\", \"wb\") as f:\n",
        "                            pickle.dump(model, f)\n",
        "                        elif args['model_name'] in ['ViT']:\n",
        "                            model.save_pretrained(args['save_path'])\n",
        "                        else: \n",
        "                            torch.save(model.state_dict(), args['save_path'])\n",
        "\n"
      ],
      "metadata": {
        "id": "okonuDVi-ZYD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch \n",
        "import torch.nn as nn \n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n",
        "\n",
        "from transformers import ViTForImageClassification, ViTModel, ViTConfig\n",
        "\n",
        "class ViTMosaiksModel(nn.Module):\n",
        "\n",
        "\tdef __init__(self, n_classes, mosaiks_dim = 64, mlp_dim = 128):\n",
        "\t\t\n",
        "\t\tsuper().__init__()\n",
        "\n",
        "\t\tself.n_classes = n_classes\n",
        "\t\t# applies pooling layer \n",
        "\t\tconfiguration = ViTConfig()\n",
        "\t\tself.model = ViTModel(configuration).from_pretrained('google/vit-base-patch16-224-in21k')\n",
        "\t\t\n",
        "\t\thidden_dim = 768\n",
        "\t\tself.mlp = nn.Sequential(\n",
        "            nn.Linear(hidden_dim + mosaiks_dim, mlp_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(mlp_dim, mlp_dim),\n",
        "            nn.ReLU(),            \n",
        "            nn.Linear(mlp_dim, self.n_classes)\n",
        "        )\n",
        "\t\n",
        "\tdef forward(self, X, mosaiks_features):\n",
        "\t\tdevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\t\tmodel_out = self.model(X)['pooler_output']\n",
        "\t\tmosaiks_features = mosaiks_features.to(device)\n",
        "\t\tconcat_output = torch.cat((model_out, mosaiks_features), dim=1)\n",
        "\t\tlogits = self.mlp(concat_output)\n",
        "\n",
        "\t\treturn logits"
      ],
      "metadata": {
        "id": "-188r83lDeD2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "args = {\n",
        "  'data_dir': 'west_africa',\n",
        "  'csv_file': 'west_africa_df', \n",
        "  'mosaiks_csv_file': 'west_africa_mosaiks_feats',\n",
        "  'mosaiks': False, \n",
        "  'outcome': 'Mean_BMI_bin',\n",
        "  'model_name':'ViT',\n",
        "\n",
        "  'val_every': 200, \n",
        "  'batch_size': 64, \n",
        "  'lr':2e-5, \n",
        "  'eps':1e-8\n",
        "}\n",
        "\n",
        "# set device to GPU if possible\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# Number of classes \n",
        "CLASSES = 3\n",
        "CLASS_NAMES = [i for i in range(CLASSES)]\n",
        "\n",
        "epoch_n = 10\n",
        "\n",
        "# read df \n",
        "df = pd.read_csv(args['csv_file'] + \".csv\")\n",
        "\n",
        "# if args.metadata and args.mosaiks: \n",
        "#     raise NotImplementedError(\"Functionality for both mosaiks and metadata has not been implemented yet.\")\n",
        "\n",
        "cnt_id_map = None\n",
        "metadata = None\n",
        "# if args.metadata: \n",
        "#     metadata = [\"country\"]\n",
        "#     unique_countries = list(set(df[\"country\"]))\n",
        "#     unique_countries_int = [int(str(ord(c[0])) + str(ord(c[1]))) for c in unique_countries]\n",
        "#     cnt_id_map = {float(v):k for k, v in enumerate(set(unique_countries_int))}\n",
        "\n",
        "# if filename is None: \n",
        "#     filename = f'./results/{args.model_name}/{datetime.now()}.txt'\n",
        "\n",
        "write_file = None\n",
        "# write_file = open(filename, \"w\")\n",
        "\n",
        "# if write_file:\n",
        "#     write_file.write(f'*** args: {args}\\n\\n')\n",
        "\n",
        "# create model \n",
        "collators, model = create_model_and_collator(\n",
        "    args = args, \n",
        "    model_name = args['model_name'], \n",
        "    metadata=metadata, cnt_id_map = cnt_id_map\n",
        "\n",
        ")\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "9_9cu8DI_Q8i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load data \n",
        "data_loaders = create_dataset(\n",
        "    args = args, collator_fns = collators, metadata = metadata, cnt_id_map = cnt_id_map\n",
        ")\n",
        "\n",
        "# train_label_stats = dataset_statistics(args, data_loaders[0])\n",
        "# val_label_stats = dataset_statistics(args, data_loaders[1])\n",
        "# print(f'*** Training set label statistics: {train_label_stats}')\n",
        "# print(f'*** Validation set label statistics: {val_label_stats}')\n",
        "\n",
        "# if write_file:\n",
        "#     write_file.write(f'*** Training set label statistics: {train_label_stats}')\n",
        "#     write_file.write(f'*** Validation set label statistics: {val_label_stats}')\t\n",
        "\n",
        "\n",
        "# if args.model_name in ['logistic_regression', 'basic_cnn', 'dense_cnn']:\n",
        "#     optim = torch.optim.Adam(params = model.parameters())\n",
        "# else: \n",
        "optim = torch.optim.AdamW(params=model.parameters(), lr=args['lr'], eps=args['eps'])\n",
        "\n",
        "total_steps = len(data_loaders[0]) * epoch_n \n",
        "scheduler = get_linear_schedule_with_warmup(optim, num_warmup_steps=0, num_training_steps = total_steps)\n",
        "\n",
        "# get class weights \n",
        "class_weights = 1 - df[args['outcome']].value_counts(normalize=True).sort_index()\n",
        "class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
        "criterion = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
        "\n",
        "if write_file: \n",
        "    write_file.write(f'\\nModel:\\n {model}\\nOptimizer:{optim}\\n')"
      ],
      "metadata": {
        "id": "fDvs-h3RCow2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle"
      ],
      "metadata": {
        "id": "gq-2uUpCH1Nm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "args['save_path'] = 'ViT_mosiaks_model.pkl'"
      ],
      "metadata": {
        "id": "TVnUNMn8HxHq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "args['lr'] = 1e-5"
      ],
      "metadata": {
        "id": "1-6K_q3el0KO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(args, data_loaders, epoch_n, model, optim, scheduler, criterion, metadata, device, write_file)\n",
        "\n",
        "if write_file:\n",
        "    write_file.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tymMqN_4vm4s",
        "outputId": "2d26bf39-be36-43b9-edb7-cc98cdb5107f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            ">>> Training starts...\n",
            "*** Epoch: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/97 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** Average Loss: 1.119106650352478\n",
            "*** Running accuracy on the train set: 0.078125\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "  0%|          | 0/17 [00:00<?, ?it/s]\u001b[A\n",
            "  6%|▌         | 1/17 [05:11<1:22:56, 311.00s/it]\u001b[A\n",
            " 12%|█▏        | 2/17 [05:12<32:15, 129.03s/it]  \u001b[A\n",
            " 18%|█▊        | 3/17 [05:14<16:32, 70.89s/it] \u001b[A\n",
            " 24%|██▎       | 4/17 [05:15<09:26, 43.54s/it]\u001b[A\n",
            " 29%|██▉       | 5/17 [05:17<05:41, 28.42s/it]\u001b[A\n",
            " 35%|███▌      | 6/17 [05:19<03:32, 19.29s/it]\u001b[A\n",
            " 41%|████      | 7/17 [05:21<02:16, 13.60s/it]\u001b[A\n",
            " 47%|████▋     | 8/17 [05:22<01:27,  9.77s/it]\u001b[A\n",
            " 53%|█████▎    | 9/17 [05:24<00:58,  7.33s/it]\u001b[A\n",
            " 59%|█████▉    | 10/17 [05:26<00:38,  5.56s/it]\u001b[A\n",
            " 65%|██████▍   | 11/17 [05:28<00:26,  4.47s/it]\u001b[A\n",
            " 71%|███████   | 12/17 [05:29<00:17,  3.59s/it]\u001b[A\n",
            " 76%|███████▋  | 13/17 [05:31<00:12,  3.00s/it]\u001b[A\n",
            " 82%|████████▏ | 14/17 [05:33<00:07,  2.60s/it]\u001b[A\n",
            " 88%|████████▊ | 15/17 [05:35<00:04,  2.45s/it]\u001b[A\n",
            " 94%|█████████▍| 16/17 [05:37<00:02,  2.43s/it]\u001b[A\n",
            "100%|██████████| 17/17 [05:39<00:00, 19.94s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** Accuracy on the Validation set: 0.17988929889298894\n",
            "*** Weighted accuracy on the Validation set: 0.36829457364341084\n",
            "*** Confusion matrix:\n",
            "[[ 57.   0.  43.]\n",
            " [365.   0. 361.]\n",
            " [120.   0. 138.]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 97/97 [11:23<00:00,  7.04s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** Epoch: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/97 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** Average Loss: 0.8845539093017578\n",
            "*** Running accuracy on the train set: 0.609375\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "  0%|          | 0/17 [00:00<?, ?it/s]\u001b[A\n",
            "  6%|▌         | 1/17 [00:01<00:27,  1.70s/it]\u001b[A\n",
            " 12%|█▏        | 2/17 [00:03<00:25,  1.72s/it]\u001b[A\n",
            " 18%|█▊        | 3/17 [00:05<00:24,  1.76s/it]\u001b[A\n",
            " 24%|██▎       | 4/17 [00:06<00:22,  1.73s/it]\u001b[A\n",
            " 29%|██▉       | 5/17 [00:08<00:20,  1.71s/it]\u001b[A\n",
            " 35%|███▌      | 6/17 [00:10<00:18,  1.71s/it]\u001b[A\n",
            " 41%|████      | 7/17 [00:12<00:17,  1.71s/it]\u001b[A\n",
            " 47%|████▋     | 8/17 [00:13<00:15,  1.71s/it]\u001b[A\n",
            " 53%|█████▎    | 9/17 [00:15<00:13,  1.71s/it]\u001b[A\n",
            " 59%|█████▉    | 10/17 [00:17<00:11,  1.71s/it]\u001b[A\n",
            " 65%|██████▍   | 11/17 [00:18<00:10,  1.72s/it]\u001b[A\n",
            " 71%|███████   | 12/17 [00:20<00:08,  1.73s/it]\u001b[A\n",
            " 76%|███████▋  | 13/17 [00:22<00:06,  1.72s/it]\u001b[A\n",
            " 82%|████████▏ | 14/17 [00:24<00:05,  1.72s/it]\u001b[A\n",
            " 88%|████████▊ | 15/17 [00:25<00:03,  1.72s/it]\u001b[A\n",
            " 94%|█████████▍| 16/17 [00:27<00:01,  1.72s/it]\u001b[A\n",
            "100%|██████████| 17/17 [00:29<00:00,  1.71s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** Accuracy on the Validation set: 0.6190036900369004\n",
            "*** Weighted accuracy on the Validation set: 0.5148749652977983\n",
            "*** Confusion matrix:\n",
            "[[ 45.  54.   1.]\n",
            " [106. 533.  87.]\n",
            " [ 16. 149.  93.]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 97/97 [05:29<00:00,  3.40s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** Epoch: 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/97 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** Average Loss: 0.766907811164856\n",
            "*** Running accuracy on the train set: 0.578125\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "  0%|          | 0/17 [00:00<?, ?it/s]\u001b[A\n",
            "  6%|▌         | 1/17 [00:01<00:27,  1.71s/it]\u001b[A\n",
            " 12%|█▏        | 2/17 [00:03<00:25,  1.72s/it]\u001b[A\n",
            " 18%|█▊        | 3/17 [00:05<00:24,  1.74s/it]\u001b[A\n",
            " 24%|██▎       | 4/17 [00:06<00:22,  1.72s/it]\u001b[A\n",
            " 29%|██▉       | 5/17 [00:08<00:20,  1.71s/it]\u001b[A\n",
            " 35%|███▌      | 6/17 [00:10<00:18,  1.71s/it]\u001b[A\n",
            " 41%|████      | 7/17 [00:11<00:17,  1.71s/it]\u001b[A\n",
            " 47%|████▋     | 8/17 [00:13<00:15,  1.71s/it]\u001b[A\n",
            " 53%|█████▎    | 9/17 [00:15<00:13,  1.70s/it]\u001b[A\n",
            " 59%|█████▉    | 10/17 [00:17<00:11,  1.69s/it]\u001b[A\n",
            " 65%|██████▍   | 11/17 [00:18<00:10,  1.69s/it]\u001b[A\n",
            " 71%|███████   | 12/17 [00:20<00:08,  1.69s/it]\u001b[A\n",
            " 76%|███████▋  | 13/17 [00:22<00:06,  1.70s/it]\u001b[A\n",
            " 82%|████████▏ | 14/17 [00:23<00:05,  1.70s/it]\u001b[A\n",
            " 88%|████████▊ | 15/17 [00:25<00:03,  1.71s/it]\u001b[A\n",
            " 94%|█████████▍| 16/17 [00:27<00:01,  1.71s/it]\u001b[A\n",
            "100%|██████████| 17/17 [00:28<00:00,  1.70s/it]\n",
            "  1%|          | 1/97 [00:31<51:05, 31.93s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** Accuracy on the Validation set: 0.6116236162361623\n",
            "*** Weighted accuracy on the Validation set: 0.5888696692079356\n",
            "*** Confusion matrix:\n",
            "[[ 50.  46.   4.]\n",
            " [102. 444. 180.]\n",
            " [ 15.  74. 169.]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 97/97 [05:27<00:00,  3.37s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** Epoch: 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/97 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** Average Loss: 0.6523650288581848\n",
            "*** Running accuracy on the train set: 0.703125\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "  0%|          | 0/17 [00:00<?, ?it/s]\u001b[A\n",
            "  6%|▌         | 1/17 [00:01<00:27,  1.74s/it]\u001b[A\n",
            " 12%|█▏        | 2/17 [00:03<00:26,  1.74s/it]\u001b[A\n",
            " 18%|█▊        | 3/17 [00:05<00:24,  1.74s/it]\u001b[A\n",
            " 24%|██▎       | 4/17 [00:06<00:22,  1.73s/it]\u001b[A\n",
            " 29%|██▉       | 5/17 [00:08<00:20,  1.71s/it]\u001b[A\n",
            " 35%|███▌      | 6/17 [00:10<00:18,  1.71s/it]\u001b[A\n",
            " 41%|████      | 7/17 [00:12<00:17,  1.71s/it]\u001b[A\n",
            " 47%|████▋     | 8/17 [00:13<00:15,  1.71s/it]\u001b[A\n",
            " 53%|█████▎    | 9/17 [00:15<00:13,  1.70s/it]\u001b[A\n",
            " 59%|█████▉    | 10/17 [00:17<00:11,  1.70s/it]\u001b[A\n",
            " 65%|██████▍   | 11/17 [00:18<00:10,  1.71s/it]\u001b[A\n",
            " 71%|███████   | 12/17 [00:20<00:08,  1.72s/it]\u001b[A\n",
            " 76%|███████▋  | 13/17 [00:22<00:06,  1.71s/it]\u001b[A\n",
            " 82%|████████▏ | 14/17 [00:23<00:05,  1.71s/it]\u001b[A\n",
            " 88%|████████▊ | 15/17 [00:25<00:03,  1.71s/it]\u001b[A\n",
            " 94%|█████████▍| 16/17 [00:27<00:01,  1.71s/it]\u001b[A\n",
            "100%|██████████| 17/17 [00:28<00:00,  1.71s/it]\n",
            "  1%|          | 1/97 [00:32<51:13, 32.02s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** Accuracy on the Validation set: 0.6070110701107011\n",
            "*** Weighted accuracy on the Validation set: 0.5707497811091892\n",
            "*** Confusion matrix:\n",
            "[[ 50.  47.   3.]\n",
            " [113. 458. 155.]\n",
            " [ 14.  94. 150.]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 97/97 [05:25<00:00,  3.35s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** Epoch: 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/97 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** Average Loss: 0.6430875062942505\n",
            "*** Running accuracy on the train set: 0.78125\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "  0%|          | 0/17 [00:00<?, ?it/s]\u001b[A\n",
            "  6%|▌         | 1/17 [00:01<00:27,  1.72s/it]\u001b[A\n",
            " 12%|█▏        | 2/17 [00:03<00:25,  1.73s/it]\u001b[A\n",
            " 18%|█▊        | 3/17 [00:05<00:24,  1.74s/it]\u001b[A\n",
            " 24%|██▎       | 4/17 [00:06<00:22,  1.71s/it]\u001b[A\n",
            " 29%|██▉       | 5/17 [00:08<00:20,  1.70s/it]\u001b[A\n",
            " 35%|███▌      | 6/17 [00:10<00:18,  1.70s/it]\u001b[A\n",
            " 41%|████      | 7/17 [00:11<00:16,  1.70s/it]\u001b[A\n",
            " 47%|████▋     | 8/17 [00:13<00:15,  1.70s/it]\u001b[A\n",
            " 53%|█████▎    | 9/17 [00:15<00:13,  1.69s/it]\u001b[A\n",
            " 59%|█████▉    | 10/17 [00:17<00:11,  1.70s/it]\u001b[A\n",
            " 65%|██████▍   | 11/17 [00:18<00:10,  1.71s/it]\u001b[A\n",
            " 71%|███████   | 12/17 [00:20<00:08,  1.72s/it]\u001b[A\n",
            " 76%|███████▋  | 13/17 [00:22<00:06,  1.72s/it]\u001b[A\n",
            " 82%|████████▏ | 14/17 [00:23<00:05,  1.72s/it]\u001b[A\n",
            " 88%|████████▊ | 15/17 [00:25<00:03,  1.72s/it]\u001b[A\n",
            " 94%|█████████▍| 16/17 [00:27<00:01,  1.73s/it]\u001b[A\n",
            "100%|██████████| 17/17 [00:28<00:00,  1.70s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** Accuracy on the Validation set: 0.6328413284132841\n",
            "*** Weighted accuracy on the Validation set: 0.5838674269118244\n",
            "*** Confusion matrix:\n",
            "[[ 64.  36.   0.]\n",
            " [149. 520.  57.]\n",
            " [ 18. 138. 102.]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 97/97 [05:28<00:00,  3.39s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** Epoch: 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/97 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** Average Loss: 0.5946272015571594\n",
            "*** Running accuracy on the train set: 0.734375\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "  0%|          | 0/17 [00:00<?, ?it/s]\u001b[A\n",
            "  6%|▌         | 1/17 [00:01<00:26,  1.69s/it]\u001b[A\n",
            " 12%|█▏        | 2/17 [00:03<00:25,  1.71s/it]\u001b[A\n",
            " 18%|█▊        | 3/17 [00:05<00:24,  1.72s/it]\u001b[A\n",
            " 24%|██▎       | 4/17 [00:06<00:22,  1.71s/it]\u001b[A\n",
            " 29%|██▉       | 5/17 [00:08<00:20,  1.71s/it]\u001b[A\n",
            " 35%|███▌      | 6/17 [00:10<00:18,  1.71s/it]\u001b[A\n",
            " 41%|████      | 7/17 [00:11<00:17,  1.71s/it]\u001b[A\n",
            " 47%|████▋     | 8/17 [00:13<00:15,  1.71s/it]\u001b[A\n",
            " 53%|█████▎    | 9/17 [00:15<00:13,  1.70s/it]\u001b[A\n",
            " 59%|█████▉    | 10/17 [00:17<00:11,  1.70s/it]\u001b[A\n",
            " 65%|██████▍   | 11/17 [00:18<00:10,  1.71s/it]\u001b[A\n",
            " 71%|███████   | 12/17 [00:20<00:08,  1.71s/it]\u001b[A\n",
            " 76%|███████▋  | 13/17 [00:22<00:06,  1.71s/it]\u001b[A\n",
            " 82%|████████▏ | 14/17 [00:23<00:05,  1.71s/it]\u001b[A\n",
            " 88%|████████▊ | 15/17 [00:25<00:03,  1.71s/it]\u001b[A\n",
            " 94%|█████████▍| 16/17 [00:27<00:01,  1.71s/it]\u001b[A\n",
            "100%|██████████| 17/17 [00:28<00:00,  1.70s/it]\n",
            "  1%|          | 1/97 [00:31<51:06, 31.94s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** Accuracy on the Validation set: 0.6217712177121771\n",
            "*** Weighted accuracy on the Validation set: 0.5503177653917611\n",
            "*** Confusion matrix:\n",
            "[[ 47.  52.   1.]\n",
            " [117. 500. 109.]\n",
            " [ 12. 119. 127.]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 97/97 [05:26<00:00,  3.37s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** Epoch: 6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/97 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** Average Loss: 0.5705587863922119\n",
            "*** Running accuracy on the train set: 0.78125\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "  0%|          | 0/17 [00:00<?, ?it/s]\u001b[A\n",
            "  6%|▌         | 1/17 [00:01<00:27,  1.70s/it]\u001b[A\n",
            " 12%|█▏        | 2/17 [00:03<00:25,  1.72s/it]\u001b[A\n",
            " 18%|█▊        | 3/17 [00:05<00:24,  1.73s/it]\u001b[A\n",
            " 24%|██▎       | 4/17 [00:06<00:22,  1.72s/it]\u001b[A\n",
            " 29%|██▉       | 5/17 [00:08<00:20,  1.72s/it]\u001b[A\n",
            " 35%|███▌      | 6/17 [00:10<00:18,  1.71s/it]\u001b[A\n",
            " 41%|████      | 7/17 [00:11<00:17,  1.71s/it]\u001b[A\n",
            " 47%|████▋     | 8/17 [00:13<00:15,  1.71s/it]\u001b[A\n",
            " 53%|█████▎    | 9/17 [00:15<00:13,  1.71s/it]\u001b[A\n",
            " 59%|█████▉    | 10/17 [00:17<00:11,  1.70s/it]\u001b[A\n",
            " 65%|██████▍   | 11/17 [00:18<00:10,  1.70s/it]\u001b[A\n",
            " 71%|███████   | 12/17 [00:20<00:08,  1.71s/it]\u001b[A\n",
            " 76%|███████▋  | 13/17 [00:22<00:06,  1.71s/it]\u001b[A\n",
            " 82%|████████▏ | 14/17 [00:23<00:05,  1.71s/it]\u001b[A\n",
            " 88%|████████▊ | 15/17 [00:25<00:03,  1.71s/it]\u001b[A\n",
            " 94%|█████████▍| 16/17 [00:27<00:01,  1.72s/it]\u001b[A\n",
            "100%|██████████| 17/17 [00:28<00:00,  1.70s/it]\n",
            "  1%|          | 1/97 [00:31<51:03, 31.91s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** Accuracy on the Validation set: 0.5821033210332104\n",
            "*** Weighted accuracy on the Validation set: 0.520776048006492\n",
            "*** Confusion matrix:\n",
            "[[ 32.  59.   9.]\n",
            " [ 84. 432. 210.]\n",
            " [  6.  85. 167.]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 97/97 [05:25<00:00,  3.36s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** Epoch: 7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/97 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** Average Loss: 0.3262180685997009\n",
            "*** Running accuracy on the train set: 0.921875\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "  0%|          | 0/17 [00:00<?, ?it/s]\u001b[A\n",
            "  6%|▌         | 1/17 [00:01<00:30,  1.93s/it]\u001b[A\n",
            " 12%|█▏        | 2/17 [00:03<00:26,  1.80s/it]\u001b[A\n",
            " 18%|█▊        | 3/17 [00:05<00:24,  1.76s/it]\u001b[A\n",
            " 24%|██▎       | 4/17 [00:07<00:22,  1.73s/it]\u001b[A\n",
            " 29%|██▉       | 5/17 [00:08<00:20,  1.71s/it]\u001b[A\n",
            " 35%|███▌      | 6/17 [00:10<00:18,  1.71s/it]\u001b[A\n",
            " 41%|████      | 7/17 [00:12<00:17,  1.71s/it]\u001b[A\n",
            " 47%|████▋     | 8/17 [00:13<00:15,  1.71s/it]\u001b[A\n",
            " 53%|█████▎    | 9/17 [00:15<00:13,  1.71s/it]\u001b[A\n",
            " 59%|█████▉    | 10/17 [00:17<00:11,  1.71s/it]\u001b[A\n",
            " 65%|██████▍   | 11/17 [00:18<00:10,  1.72s/it]\u001b[A\n",
            " 71%|███████   | 12/17 [00:20<00:08,  1.72s/it]\u001b[A\n",
            " 76%|███████▋  | 13/17 [00:22<00:06,  1.72s/it]\u001b[A\n",
            " 82%|████████▏ | 14/17 [00:24<00:05,  1.72s/it]\u001b[A\n",
            " 88%|████████▊ | 15/17 [00:25<00:03,  1.74s/it]\u001b[A\n",
            " 94%|█████████▍| 16/17 [00:27<00:01,  1.73s/it]\u001b[A\n",
            "100%|██████████| 17/17 [00:29<00:00,  1.72s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** Accuracy on the Validation set: 0.6485239852398524\n",
            "*** Weighted accuracy on the Validation set: 0.5194422021483333\n",
            "*** Confusion matrix:\n",
            "[[ 38.  60.   2.]\n",
            " [ 98. 560.  68.]\n",
            " [  9. 144. 105.]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 97/97 [05:28<00:00,  3.39s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** Epoch: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/97 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** Average Loss: 0.28685882687568665\n",
            "*** Running accuracy on the train set: 0.921875\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "  0%|          | 0/17 [00:00<?, ?it/s]\u001b[A\n",
            "  6%|▌         | 1/17 [00:01<00:26,  1.69s/it]\u001b[A\n",
            " 12%|█▏        | 2/17 [00:03<00:25,  1.72s/it]\u001b[A\n",
            " 18%|█▊        | 3/17 [00:05<00:24,  1.74s/it]\u001b[A\n",
            " 24%|██▎       | 4/17 [00:06<00:22,  1.73s/it]\u001b[A\n",
            " 29%|██▉       | 5/17 [00:08<00:20,  1.73s/it]\u001b[A\n",
            " 35%|███▌      | 6/17 [00:10<00:19,  1.73s/it]\u001b[A\n",
            " 41%|████      | 7/17 [00:12<00:17,  1.73s/it]\u001b[A\n",
            " 47%|████▋     | 8/17 [00:13<00:15,  1.73s/it]\u001b[A\n",
            " 53%|█████▎    | 9/17 [00:15<00:13,  1.73s/it]\u001b[A\n",
            " 59%|█████▉    | 10/17 [00:17<00:12,  1.74s/it]\u001b[A\n",
            " 65%|██████▍   | 11/17 [00:19<00:10,  1.74s/it]\u001b[A\n",
            " 71%|███████   | 12/17 [00:20<00:08,  1.73s/it]\u001b[A\n",
            " 76%|███████▋  | 13/17 [00:22<00:06,  1.74s/it]\u001b[A\n",
            " 82%|████████▏ | 14/17 [00:24<00:05,  1.73s/it]\u001b[A\n",
            " 88%|████████▊ | 15/17 [00:25<00:03,  1.73s/it]\u001b[A\n",
            " 94%|█████████▍| 16/17 [00:27<00:01,  1.72s/it]\u001b[A\n",
            "100%|██████████| 17/17 [00:29<00:00,  1.72s/it]\n",
            "  1%|          | 1/97 [00:32<51:35, 32.25s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** Accuracy on the Validation set: 0.6254612546125461\n",
            "*** Weighted accuracy on the Validation set: 0.5182843231469024\n",
            "*** Confusion matrix:\n",
            "[[ 30.  65.   5.]\n",
            " [ 77. 503. 146.]\n",
            " [  5. 108. 145.]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 97/97 [05:26<00:00,  3.36s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** Epoch: 9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/97 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** Average Loss: 0.171013742685318\n",
            "*** Running accuracy on the train set: 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "  0%|          | 0/17 [00:00<?, ?it/s]\u001b[A\n",
            "  6%|▌         | 1/17 [00:01<00:27,  1.73s/it]\u001b[A\n",
            " 12%|█▏        | 2/17 [00:03<00:26,  1.76s/it]\u001b[A\n",
            " 18%|█▊        | 3/17 [00:05<00:24,  1.76s/it]\u001b[A\n",
            " 24%|██▎       | 4/17 [00:07<00:24,  1.90s/it]\u001b[A\n",
            " 29%|██▉       | 5/17 [00:09<00:23,  1.92s/it]\u001b[A\n",
            " 35%|███▌      | 6/17 [00:11<00:20,  1.91s/it]\u001b[A\n",
            " 41%|████      | 7/17 [00:12<00:18,  1.84s/it]\u001b[A\n",
            " 47%|████▋     | 8/17 [00:14<00:16,  1.81s/it]\u001b[A\n",
            " 53%|█████▎    | 9/17 [00:16<00:14,  1.77s/it]\u001b[A\n",
            " 59%|█████▉    | 10/17 [00:18<00:12,  1.75s/it]\u001b[A\n",
            " 65%|██████▍   | 11/17 [00:19<00:10,  1.74s/it]\u001b[A\n",
            " 71%|███████   | 12/17 [00:21<00:08,  1.74s/it]\u001b[A\n",
            " 76%|███████▋  | 13/17 [00:23<00:06,  1.72s/it]\u001b[A\n",
            " 82%|████████▏ | 14/17 [00:24<00:05,  1.72s/it]\u001b[A\n",
            " 88%|████████▊ | 15/17 [00:26<00:03,  1.71s/it]\u001b[A\n",
            " 94%|█████████▍| 16/17 [00:28<00:01,  1.70s/it]\u001b[A\n",
            "100%|██████████| 17/17 [00:29<00:00,  1.76s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** Accuracy on the Validation set: 0.6494464944649446\n",
            "*** Weighted accuracy on the Validation set: 0.4814914472419758\n",
            "*** Confusion matrix:\n",
            "[[ 20.  76.   4.]\n",
            " [ 56. 563. 107.]\n",
            " [  3. 134. 121.]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 97/97 [05:28<00:00,  3.39s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train(args, data_loaders, epoch_n, model, optim, scheduler, criterion, metadata, device, write_file)\n",
        "\n",
        "if write_file:\n",
        "    write_file.close()"
      ],
      "metadata": {
        "id": "NibEUo2oDF4u",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a56cbe9a-1827-488d-8aa9-0b8d95d90f20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            ">>> Training starts...\n",
            "*** Epoch: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/97 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** Average Loss: 1.1114825010299683\n",
            "*** Running accuracy on the train set: 0.328125\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "  0%|          | 0/17 [00:00<?, ?it/s]\u001b[A\n",
            "  6%|▌         | 1/17 [00:27<07:23, 27.74s/it]\u001b[A\n",
            " 12%|█▏        | 2/17 [00:29<03:03, 12.24s/it]\u001b[A\n",
            " 18%|█▊        | 3/17 [00:30<01:42,  7.30s/it]\u001b[A\n",
            " 24%|██▎       | 4/17 [00:31<01:04,  4.98s/it]\u001b[A\n",
            " 29%|██▉       | 5/17 [00:33<00:44,  3.70s/it]\u001b[A\n",
            " 35%|███▌      | 6/17 [00:34<00:32,  2.93s/it]\u001b[A\n",
            " 41%|████      | 7/17 [00:36<00:24,  2.45s/it]\u001b[A\n",
            " 47%|████▋     | 8/17 [00:37<00:19,  2.15s/it]\u001b[A\n",
            " 53%|█████▎    | 9/17 [00:39<00:16,  2.07s/it]\u001b[A\n",
            " 59%|█████▉    | 10/17 [00:41<00:13,  1.89s/it]\u001b[A\n",
            " 65%|██████▍   | 11/17 [00:42<00:10,  1.76s/it]\u001b[A\n",
            " 71%|███████   | 12/17 [00:44<00:08,  1.67s/it]\u001b[A\n",
            " 76%|███████▋  | 13/17 [00:45<00:06,  1.61s/it]\u001b[A\n",
            " 82%|████████▏ | 14/17 [00:46<00:04,  1.54s/it]\u001b[A\n",
            " 88%|████████▊ | 15/17 [00:48<00:02,  1.48s/it]\u001b[A\n",
            " 94%|█████████▍| 16/17 [00:49<00:01,  1.47s/it]\u001b[A\n",
            "100%|██████████| 17/17 [00:51<00:00,  3.01s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** Accuracy on the Validation set: 0.3284132841328413\n",
            "*** Weighted accuracy on the Validation set: 0.4130311455612661\n",
            "*** Confusion matrix:\n",
            "[[ 48.  34.  38.]\n",
            " [165. 152. 398.]\n",
            " [ 43.  50. 156.]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 97/97 [05:34<00:00,  3.44s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** Epoch: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/97 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** Average Loss: 0.6878499388694763\n",
            "*** Running accuracy on the train set: 0.734375\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "  0%|          | 0/17 [00:00<?, ?it/s]\u001b[A\n",
            "  6%|▌         | 1/17 [00:01<00:24,  1.55s/it]\u001b[A\n",
            " 12%|█▏        | 2/17 [00:03<00:23,  1.57s/it]\u001b[A\n",
            " 18%|█▊        | 3/17 [00:04<00:22,  1.58s/it]\u001b[A\n",
            " 24%|██▎       | 4/17 [00:06<00:20,  1.59s/it]\u001b[A\n",
            " 29%|██▉       | 5/17 [00:07<00:19,  1.59s/it]\u001b[A\n",
            " 35%|███▌      | 6/17 [00:09<00:17,  1.58s/it]\u001b[A\n",
            " 41%|████      | 7/17 [00:11<00:15,  1.58s/it]\u001b[A\n",
            " 47%|████▋     | 8/17 [00:12<00:14,  1.58s/it]\u001b[A\n",
            " 53%|█████▎    | 9/17 [00:14<00:12,  1.59s/it]\u001b[A\n",
            " 59%|█████▉    | 10/17 [00:15<00:11,  1.60s/it]\u001b[A\n",
            " 65%|██████▍   | 11/17 [00:17<00:09,  1.61s/it]\u001b[A\n",
            " 71%|███████   | 12/17 [00:19<00:08,  1.67s/it]\u001b[A\n",
            " 76%|███████▋  | 13/17 [00:21<00:07,  1.77s/it]\u001b[A\n",
            " 82%|████████▏ | 14/17 [00:23<00:05,  1.75s/it]\u001b[A\n",
            " 88%|████████▊ | 15/17 [00:24<00:03,  1.76s/it]\u001b[A\n",
            " 94%|█████████▍| 16/17 [00:26<00:01,  1.71s/it]\u001b[A\n",
            "100%|██████████| 17/17 [00:27<00:00,  1.63s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** Accuracy on the Validation set: 0.6494464944649446\n",
            "*** Weighted accuracy on the Validation set: 0.5715982812368355\n",
            "*** Confusion matrix:\n",
            "[[ 60.  59.   1.]\n",
            " [ 83. 524. 108.]\n",
            " [ 13. 116. 120.]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 97/97 [05:10<00:00,  3.20s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** Epoch: 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/97 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** Average Loss: 0.74836665391922\n",
            "*** Running accuracy on the train set: 0.65625\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "  0%|          | 0/17 [00:00<?, ?it/s]\u001b[A\n",
            "  6%|▌         | 1/17 [00:01<00:24,  1.53s/it]\u001b[A\n",
            " 12%|█▏        | 2/17 [00:03<00:23,  1.54s/it]\u001b[A\n",
            " 18%|█▊        | 3/17 [00:04<00:21,  1.56s/it]\u001b[A\n",
            " 24%|██▎       | 4/17 [00:06<00:20,  1.58s/it]\u001b[A\n",
            " 29%|██▉       | 5/17 [00:07<00:18,  1.58s/it]\u001b[A\n",
            " 35%|███▌      | 6/17 [00:09<00:17,  1.58s/it]\u001b[A\n",
            " 41%|████      | 7/17 [00:11<00:15,  1.59s/it]\u001b[A\n",
            " 47%|████▋     | 8/17 [00:12<00:14,  1.57s/it]\u001b[A\n",
            " 53%|█████▎    | 9/17 [00:14<00:12,  1.57s/it]\u001b[A\n",
            " 59%|█████▉    | 10/17 [00:15<00:11,  1.58s/it]\u001b[A\n",
            " 65%|██████▍   | 11/17 [00:17<00:09,  1.58s/it]\u001b[A\n",
            " 71%|███████   | 12/17 [00:18<00:07,  1.58s/it]\u001b[A\n",
            " 76%|███████▋  | 13/17 [00:20<00:06,  1.58s/it]\u001b[A\n",
            " 82%|████████▏ | 14/17 [00:22<00:04,  1.57s/it]\u001b[A\n",
            " 88%|████████▊ | 15/17 [00:23<00:03,  1.56s/it]\u001b[A\n",
            " 94%|█████████▍| 16/17 [00:25<00:01,  1.56s/it]\u001b[A\n",
            "100%|██████████| 17/17 [00:26<00:00,  1.56s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** Accuracy on the Validation set: 0.6614391143911439\n",
            "*** Weighted accuracy on the Validation set: 0.5689791801237584\n",
            "*** Confusion matrix:\n",
            "[[ 57.  61.   2.]\n",
            " [ 80. 542.  93.]\n",
            " [ 17. 114. 118.]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 97/97 [05:08<00:00,  3.18s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** Epoch: 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/97 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** Average Loss: 0.644568145275116\n",
            "*** Running accuracy on the train set: 0.75\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "  0%|          | 0/17 [00:00<?, ?it/s]\u001b[A\n",
            "  6%|▌         | 1/17 [00:01<00:24,  1.52s/it]\u001b[A\n",
            " 12%|█▏        | 2/17 [00:03<00:22,  1.52s/it]\u001b[A\n",
            " 18%|█▊        | 3/17 [00:04<00:21,  1.55s/it]\u001b[A\n",
            " 24%|██▎       | 4/17 [00:06<00:20,  1.56s/it]\u001b[A\n",
            " 29%|██▉       | 5/17 [00:07<00:18,  1.56s/it]\u001b[A\n",
            " 35%|███▌      | 6/17 [00:09<00:17,  1.60s/it]\u001b[A\n",
            " 41%|████      | 7/17 [00:11<00:15,  1.59s/it]\u001b[A\n",
            " 47%|████▋     | 8/17 [00:12<00:14,  1.58s/it]\u001b[A\n",
            " 53%|█████▎    | 9/17 [00:14<00:12,  1.57s/it]\u001b[A\n",
            " 59%|█████▉    | 10/17 [00:15<00:11,  1.58s/it]\u001b[A\n",
            " 65%|██████▍   | 11/17 [00:17<00:09,  1.57s/it]\u001b[A\n",
            " 71%|███████   | 12/17 [00:18<00:07,  1.56s/it]\u001b[A\n",
            " 76%|███████▋  | 13/17 [00:20<00:06,  1.56s/it]\u001b[A\n",
            " 82%|████████▏ | 14/17 [00:21<00:04,  1.56s/it]\u001b[A\n",
            " 88%|████████▊ | 15/17 [00:23<00:03,  1.55s/it]\u001b[A\n",
            " 94%|█████████▍| 16/17 [00:24<00:01,  1.54s/it]\u001b[A\n",
            "100%|██████████| 17/17 [00:26<00:00,  1.55s/it]\n",
            "  1%|          | 1/97 [00:29<46:43, 29.21s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** Accuracy on the Validation set: 0.6383763837638377\n",
            "*** Weighted accuracy on the Validation set: 0.5990228981192837\n",
            "*** Confusion matrix:\n",
            "[[ 69.  47.   4.]\n",
            " [ 97. 489. 129.]\n",
            " [ 18.  97. 134.]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 97/97 [05:05<00:00,  3.15s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** Epoch: 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/97 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** Average Loss: 0.7508125901222229\n",
            "*** Running accuracy on the train set: 0.671875\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "  0%|          | 0/17 [00:00<?, ?it/s]\u001b[A\n",
            "  6%|▌         | 1/17 [00:01<00:24,  1.52s/it]\u001b[A\n",
            " 12%|█▏        | 2/17 [00:03<00:23,  1.55s/it]\u001b[A\n",
            " 18%|█▊        | 3/17 [00:04<00:21,  1.55s/it]\u001b[A\n",
            " 24%|██▎       | 4/17 [00:06<00:20,  1.56s/it]\u001b[A\n",
            " 29%|██▉       | 5/17 [00:07<00:18,  1.55s/it]\u001b[A\n",
            " 35%|███▌      | 6/17 [00:09<00:16,  1.54s/it]\u001b[A\n",
            " 41%|████      | 7/17 [00:10<00:15,  1.55s/it]\u001b[A\n",
            " 47%|████▋     | 8/17 [00:12<00:14,  1.56s/it]\u001b[A\n",
            " 53%|█████▎    | 9/17 [00:13<00:12,  1.56s/it]\u001b[A\n",
            " 59%|█████▉    | 10/17 [00:15<00:10,  1.57s/it]\u001b[A\n",
            " 65%|██████▍   | 11/17 [00:17<00:09,  1.57s/it]\u001b[A\n",
            " 71%|███████   | 12/17 [00:18<00:07,  1.56s/it]\u001b[A\n",
            " 76%|███████▋  | 13/17 [00:20<00:06,  1.56s/it]\u001b[A\n",
            " 82%|████████▏ | 14/17 [00:21<00:04,  1.55s/it]\u001b[A\n",
            " 88%|████████▊ | 15/17 [00:23<00:03,  1.54s/it]\u001b[A\n",
            " 94%|█████████▍| 16/17 [00:24<00:01,  1.54s/it]\u001b[A\n",
            "100%|██████████| 17/17 [00:26<00:00,  1.54s/it]\n",
            "  1%|          | 1/97 [00:29<46:25, 29.02s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** Accuracy on the Validation set: 0.6134686346863468\n",
            "*** Weighted accuracy on the Validation set: 0.5654609112440437\n",
            "*** Confusion matrix:\n",
            "[[ 52.  64.   4.]\n",
            " [ 79. 458. 178.]\n",
            " [ 13.  81. 155.]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 97/97 [05:06<00:00,  3.16s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** Epoch: 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/97 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** Average Loss: 0.39360618591308594\n",
            "*** Running accuracy on the train set: 0.953125\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "  0%|          | 0/17 [00:00<?, ?it/s]\u001b[A\n",
            "  6%|▌         | 1/17 [00:01<00:24,  1.52s/it]\u001b[A\n",
            " 12%|█▏        | 2/17 [00:03<00:23,  1.55s/it]\u001b[A\n",
            " 18%|█▊        | 3/17 [00:04<00:21,  1.56s/it]\u001b[A\n",
            " 24%|██▎       | 4/17 [00:06<00:20,  1.56s/it]\u001b[A\n",
            " 29%|██▉       | 5/17 [00:07<00:18,  1.56s/it]\u001b[A\n",
            " 35%|███▌      | 6/17 [00:09<00:17,  1.56s/it]\u001b[A\n",
            " 41%|████      | 7/17 [00:10<00:15,  1.56s/it]\u001b[A\n",
            " 47%|████▋     | 8/17 [00:12<00:13,  1.55s/it]\u001b[A\n",
            " 53%|█████▎    | 9/17 [00:13<00:12,  1.55s/it]\u001b[A\n",
            " 59%|█████▉    | 10/17 [00:15<00:10,  1.57s/it]\u001b[A\n",
            " 65%|██████▍   | 11/17 [00:17<00:09,  1.55s/it]\u001b[A\n",
            " 71%|███████   | 12/17 [00:18<00:07,  1.55s/it]\u001b[A\n",
            " 76%|███████▋  | 13/17 [00:20<00:06,  1.55s/it]\u001b[A\n",
            " 82%|████████▏ | 14/17 [00:21<00:04,  1.54s/it]\u001b[A\n",
            " 88%|████████▊ | 15/17 [00:23<00:03,  1.53s/it]\u001b[A\n",
            " 94%|█████████▍| 16/17 [00:24<00:01,  1.54s/it]\u001b[A\n",
            "100%|██████████| 17/17 [00:26<00:00,  1.54s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** Accuracy on the Validation set: 0.6715867158671587\n",
            "*** Weighted accuracy on the Validation set: 0.49093577105625297\n",
            "*** Confusion matrix:\n",
            "[[ 18. 100.   2.]\n",
            " [ 18. 584. 113.]\n",
            " [  2. 121. 126.]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 97/97 [05:08<00:00,  3.19s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** Epoch: 6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/97 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** Average Loss: 0.47175732254981995\n",
            "*** Running accuracy on the train set: 0.8125\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "  0%|          | 0/17 [00:00<?, ?it/s]\u001b[A\n",
            "  6%|▌         | 1/17 [00:01<00:24,  1.52s/it]\u001b[A\n",
            " 12%|█▏        | 2/17 [00:03<00:23,  1.55s/it]\u001b[A\n",
            " 18%|█▊        | 3/17 [00:04<00:21,  1.56s/it]\u001b[A\n",
            " 24%|██▎       | 4/17 [00:06<00:20,  1.57s/it]\u001b[A\n",
            " 29%|██▉       | 5/17 [00:07<00:18,  1.57s/it]\u001b[A\n",
            " 35%|███▌      | 6/17 [00:09<00:17,  1.57s/it]\u001b[A\n",
            " 41%|████      | 7/17 [00:10<00:15,  1.56s/it]\u001b[A\n",
            " 47%|████▋     | 8/17 [00:12<00:13,  1.55s/it]\u001b[A\n",
            " 53%|█████▎    | 9/17 [00:14<00:12,  1.56s/it]\u001b[A\n",
            " 59%|█████▉    | 10/17 [00:15<00:10,  1.56s/it]\u001b[A\n",
            " 65%|██████▍   | 11/17 [00:17<00:09,  1.56s/it]\u001b[A\n",
            " 71%|███████   | 12/17 [00:18<00:07,  1.55s/it]\u001b[A\n",
            " 76%|███████▋  | 13/17 [00:20<00:06,  1.54s/it]\u001b[A\n",
            " 82%|████████▏ | 14/17 [00:21<00:04,  1.53s/it]\u001b[A\n",
            " 88%|████████▊ | 15/17 [00:23<00:03,  1.53s/it]\u001b[A\n",
            " 94%|█████████▍| 16/17 [00:24<00:01,  1.53s/it]\u001b[A\n",
            "100%|██████████| 17/17 [00:26<00:00,  1.54s/it]\n",
            "  1%|          | 1/97 [00:28<46:18, 28.94s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** Accuracy on the Validation set: 0.6023985239852399\n",
            "*** Weighted accuracy on the Validation set: 0.5319120304060063\n",
            "*** Confusion matrix:\n",
            "[[ 35.  75.  10.]\n",
            " [ 48. 450. 217.]\n",
            " [  6.  75. 168.]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 97/97 [05:04<00:00,  3.14s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** Epoch: 7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/97 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** Average Loss: 0.3748129904270172\n",
            "*** Running accuracy on the train set: 0.90625\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "  0%|          | 0/17 [00:00<?, ?it/s]\u001b[A\n",
            "  6%|▌         | 1/17 [00:01<00:24,  1.51s/it]\u001b[A\n",
            " 12%|█▏        | 2/17 [00:03<00:23,  1.53s/it]\u001b[A\n",
            " 18%|█▊        | 3/17 [00:04<00:21,  1.54s/it]\u001b[A\n",
            " 24%|██▎       | 4/17 [00:06<00:19,  1.53s/it]\u001b[A\n",
            " 29%|██▉       | 5/17 [00:07<00:18,  1.53s/it]\u001b[A\n",
            " 35%|███▌      | 6/17 [00:09<00:16,  1.53s/it]\u001b[A\n",
            " 41%|████      | 7/17 [00:10<00:15,  1.54s/it]\u001b[A\n",
            " 47%|████▋     | 8/17 [00:12<00:13,  1.54s/it]\u001b[A\n",
            " 53%|█████▎    | 9/17 [00:13<00:12,  1.54s/it]\u001b[A\n",
            " 59%|█████▉    | 10/17 [00:15<00:10,  1.55s/it]\u001b[A\n",
            " 65%|██████▍   | 11/17 [00:16<00:09,  1.55s/it]\u001b[A\n",
            " 71%|███████   | 12/17 [00:18<00:07,  1.54s/it]\u001b[A\n",
            " 76%|███████▋  | 13/17 [00:19<00:06,  1.54s/it]\u001b[A\n",
            " 82%|████████▏ | 14/17 [00:21<00:04,  1.53s/it]\u001b[A\n",
            " 88%|████████▊ | 15/17 [00:23<00:03,  1.53s/it]\u001b[A\n",
            " 94%|█████████▍| 16/17 [00:24<00:01,  1.53s/it]\u001b[A\n",
            "100%|██████████| 17/17 [00:25<00:00,  1.53s/it]\n",
            "  1%|          | 1/97 [00:28<46:02, 28.77s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** Accuracy on the Validation set: 0.6605166051660517\n",
            "*** Weighted accuracy on the Validation set: 0.5155622489959839\n",
            "*** Confusion matrix:\n",
            "[[ 39.  79.   2.]\n",
            " [ 54. 572.  89.]\n",
            " [  6. 138. 105.]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 97/97 [05:04<00:00,  3.13s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** Epoch: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/97 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** Average Loss: 0.24936728179454803\n",
            "*** Running accuracy on the train set: 0.9375\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "  0%|          | 0/17 [00:00<?, ?it/s]\u001b[A\n",
            "  6%|▌         | 1/17 [00:01<00:24,  1.51s/it]\u001b[A\n",
            " 12%|█▏        | 2/17 [00:03<00:22,  1.52s/it]\u001b[A\n",
            " 18%|█▊        | 3/17 [00:04<00:21,  1.53s/it]\u001b[A\n",
            " 24%|██▎       | 4/17 [00:06<00:20,  1.55s/it]\u001b[A\n",
            " 29%|██▉       | 5/17 [00:07<00:18,  1.54s/it]\u001b[A\n",
            " 35%|███▌      | 6/17 [00:09<00:16,  1.53s/it]\u001b[A\n",
            " 41%|████      | 7/17 [00:10<00:15,  1.54s/it]\u001b[A\n",
            " 47%|████▋     | 8/17 [00:12<00:13,  1.53s/it]\u001b[A\n",
            " 53%|█████▎    | 9/17 [00:13<00:12,  1.53s/it]\u001b[A\n",
            " 59%|█████▉    | 10/17 [00:15<00:10,  1.54s/it]\u001b[A\n",
            " 65%|██████▍   | 11/17 [00:16<00:09,  1.54s/it]\u001b[A\n",
            " 71%|███████   | 12/17 [00:18<00:07,  1.53s/it]\u001b[A\n",
            " 76%|███████▋  | 13/17 [00:19<00:06,  1.53s/it]\u001b[A\n",
            " 82%|████████▏ | 14/17 [00:21<00:04,  1.52s/it]\u001b[A\n",
            " 88%|████████▊ | 15/17 [00:22<00:03,  1.53s/it]\u001b[A\n",
            " 94%|█████████▍| 16/17 [00:24<00:01,  1.53s/it]\u001b[A\n",
            "100%|██████████| 17/17 [00:25<00:00,  1.53s/it]\n",
            "  1%|          | 1/97 [00:28<45:55, 28.70s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** Accuracy on the Validation set: 0.6605166051660517\n",
            "*** Weighted accuracy on the Validation set: 0.49375005850909465\n",
            "*** Confusion matrix:\n",
            "[[ 39.  79.   2.]\n",
            " [ 52. 597.  66.]\n",
            " [  5. 164.  80.]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  3%|▎         | 3/97 [00:35<18:39, 11.91s/it]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-00dd527f2487>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_loaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrite_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mwrite_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mwrite_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-84bbe9f24425>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(args, data_loaders, epoch_n, model, optim, scheduler, criterion, metadata, device, write_file)\u001b[0m\n\u001b[1;32m    286\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m             \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m             \u001b[0mcorrect_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeasure_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    289\u001b[0m             \u001b[0mtotal_correct\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcorrect_n\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m             \u001b[0mtotal_sample\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0msample_n\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ye9xa2wSH41A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dA9IfFyHr2fd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## MOSAIKS RESULTS"
      ],
      "metadata": {
        "id": "dNxhpqf6r2jS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"west_africa_df.csv\")"
      ],
      "metadata": {
        "id": "ywFDCUCgVLod"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "indices = np.load(\"indices_perm2.npy\")\n",
        "n_val =int(np.floor(len(indices) * 0.15))"
      ],
      "metadata": {
        "id": "1ovVFpDuVLsq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dhsids = pd.Series(np.array(\n",
        "    data_loaders[0].dataset.dataset.image_paths\n",
        ")[indices[:-n_val]]).apply(lambda x: x.split('.')[0])\n",
        "train_df = df.loc[df[\"DHSID\"].isin(train_dhsids)]\n",
        "val_df = df.loc[~df[\"DHSID\"].isin(train_dhsids)]"
      ],
      "metadata": {
        "id": "9JmWUg6ZVOlj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression"
      ],
      "metadata": {
        "id": "wnmD9Y82YYHZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mosaiks_df = pd.read_csv(\"west_africa_mosaiks_feats.csv\")"
      ],
      "metadata": {
        "id": "NXlTwPEjXaPc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_mosaiks = mosaiks_df.loc[mosaiks_df['DHSID'].isin(train_dhsids)]\n",
        "val_mosaiks = mosaiks_df.loc[~mosaiks_df['DHSID'].isin(train_dhsids)]"
      ],
      "metadata": {
        "id": "oVKmMrKoX3_-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mosaiks_feats = [\" .\" + str(i + 1) for i in range(3999)]"
      ],
      "metadata": {
        "id": "drM8piRTY8kg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_mosaiks_fts = train_mosaiks[mosaiks_feats]\n",
        "val_mosaiks_fts = val_mosaiks[mosaiks_feats]"
      ],
      "metadata": {
        "id": "4evXv0KVZFje"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MEAN BMI BIN\n",
        "lr = LogisticRegression(random_state = 231, class_weight = 'balanced', max_iter = 1000, tol = 0.001)\n",
        "lr.fit(train_mosaiks_fts, train_df[\"Mean_BMI_bin\"])\n",
        "preds = lr.predict(val_mosaiks_fts)\n",
        "cm = confusion_matrix(val_df[\"Mean_BMI_bin\"], preds, labels=CLASS_NAMES)\n",
        "np.mean(\n",
        "    np.diag(cm) / np.sum(cm, 1)\n",
        ")"
      ],
      "metadata": {
        "id": "lM39jWO-hlXA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cm = confusion_matrix(val_df[\"Mean_BMI_bin\"], preds, labels=CLASS_NAMES)\n",
        "np.mean(\n",
        "    np.diag(cm) / np.sum(cm, 1)\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42cmsDdpawCY",
        "outputId": "58a2951e-274d-4246-a7ae-2260e29d5ae9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.46771063281824876"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# UNDER 5 BIN\n",
        "u5_lr = LogisticRegression(random_state = 231, class_weight = 'balanced', max_iter = 2000)\n",
        "u5_lr.fit(train_mosaiks_fts, train_df[\"Under5_Mortality_Rate_bin\"])\n",
        "preds = u5_lr.predict(val_mosaiks_fts)\n",
        "cm = confusion_matrix(val_df[\"Under5_Mortality_Rate_bin\"], preds, labels=CLASS_NAMES)\n",
        "np.mean(\n",
        "    np.diag(cm) / np.sum(cm, 1)\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jKDI-lXvZJ5N",
        "outputId": "3ec69fc2-d2e3-46bf-fc01-76feb8f57fe9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.3884044720511475"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# UNDER 5 BIN QUINT\n",
        "u5q_lr = LogisticRegression(random_state = 231, class_weight = 'balanced', max_iter = 4000)\n",
        "u5q_lr.fit(train_mosaiks_fts, train_df[\"Under5_Mortality_Rate_bin_quint\"])\n",
        "preds = u5q_lr.predict(val_mosaiks_fts)\n",
        "cm = confusion_matrix(val_df[\"Under5_Mortality_Rate_bin_quint\"], preds)\n",
        "np.mean(\n",
        "    np.diag(cm) / np.sum(cm, 1)\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3pdr_L5vfFqj",
        "outputId": "d60ade42-e21a-4ca8-ad9f-48f5abedbdd4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.24587755444298157"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# MEAN BMI QUINT\n",
        "bmiq_lr = LogisticRegression(random_state = 231, class_weight = 'balanced', max_iter = 4000)\n",
        "bmiq_lr.fit(train_mosaiks_fts, train_df[\"Mean_BMI_bin_quint\"])\n",
        "preds = bmiq_lr.predict(val_mosaiks_fts)\n",
        "cm = confusion_matrix(val_df[\"Mean_BMI_bin_quint\"], preds)\n",
        "np.mean(\n",
        "    np.diag(cm) / np.sum(cm, 1)\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ggb4YyeFkwq7",
        "outputId": "ae9cd981-3341-4005-a38c-a68ac492408f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.25441700648433796"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# UNDER 5 BIN QUINT\n",
        "u5q_lr = LogisticRegression(random_state = 231, class_weight = 'balanced', max_iter = 4000, C = 0.1)\n",
        "u5q_lr.fit(train_mosaiks_fts, train_df[\"Under5_Mortality_Rate_bin_quint\"])\n",
        "preds = u5q_lr.predict(val_mosaiks_fts)\n",
        "cm = confusion_matrix(val_df[\"Under5_Mortality_Rate_bin_quint\"], preds)\n",
        "np.mean(\n",
        "    np.diag(cm) / np.sum(cm, 1)\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mqn0_g0wuthu",
        "outputId": "bab91423-ef36-4ccf-f9fc-9766f737883d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.24932198275131584"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    }
  ]
}